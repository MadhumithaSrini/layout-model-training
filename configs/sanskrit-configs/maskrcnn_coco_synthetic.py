# -*- coding: utf-8 -*-
"""maskRCNN_X101_SyntheticConfig.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19jKTkljmtAvpI6sGk1t3L1TGZaLb00KL
"""

# -*- coding: utf-8 -*-
"""faster_RCNN_finetuned_COCO_config.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kFWNxuXuZkTaS4ub2C--g8h4hhH-uo5t
"""

# if your dataset is in COCO format, this cell can be replaced by the following three lines:
# from detectron2.data.datasets import register_coco_instances
# register_coco_instances("my_dataset_train", {}, "json_annotation_train.json", "path/to/image/dir")
# register_coco_instances("my_dataset_val", {}, "json_annotation_val.json", "path/to/image/dir")
import numpy as np
import os, json, cv2, random
from detectron2.structures import BoxMode
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog
from detectron2.data.catalog import DatasetCatalog
from detectron2.utils.logger import setup_logger

def get_layout_dicts(img_dir):
    bbxs =[]
    list(map(bbxs.extend, [[bbx for  bbx in fls if  bbx.endswith(".bbx")  ]  for _,_,fls in os.walk(img_dir)  ]))
    dataset_dicts = []
    for bb in bbxs:
      dir_name = bb.split('.')[0]
      print(dir_name)
      imgs_anns = json.load(open(os.path.join(img_dir, bb)))

      
      for v in list(list(imgs_anns.values())[2].keys()):
          record = {}
          id = dir_name+"-"+str(v.split('.')[0].split('-')[-1])
          #print(id,dir_name,v)
          label_dict={'Text':0,'Math':1,'Table':2,'Image':3}
          filename = v
          image_path = os.path.join(img_dir,dir_name, filename)
          
          height, width = cv2.imread(image_path).shape[:2]
          

          record["file_name"] = image_path
          record["image_id"] = id
          record["height"] = height
          record["width"] = width
          record["imgname"] = filename

        
          annos = imgs_anns['images'][v]['annotations']
          label = [(annos[r]["label"]) for r in range(len(annos))]
          label_ids = [label_dict[a] for a in label]
          #record["class_ids"] = label_ids
          objs = []
          for j, anno in enumerate(annos):
              #assert not anno["region_attributes"]
              annot = anno["bbox"]
              x1 = int(annot["xmin"]*width)
              x2 = int(annot["xmax"]*width)
              y1 = int(annot["ymin"]*height)
              y2 = int(annot["ymax"]*height)
              poly  = [x1+0.5,y1+0.5,x2+0.5,y1+0.5,x2+0.5,y2+0.5,x1+0.5,y2+0.5]
              ids = label_ids[j]
              
              obj = {
                  "bbox": [x1, y1, x2, y2],
                  "bbox_mode": BoxMode.XYXY_ABS,
                  "segmentation": [poly],
                  "category_id": ids,
              }
              objs.append(obj)
          record["annotations"] = objs
          dataset_dicts.append(record)
    return dataset_dicts

for d in ["train", "val", "test"]:
    DatasetCatalog.register("layout_" + d, lambda d=d: get_layout_dicts("data/sanskrit_synthetic/" + d)) # set the path of the dataset
    MetadataCatalog.get("layout_" + d).set(thing_classes=["Text","Math","Table","Image"])
layout_metadata = MetadataCatalog.get("layout_train")

from detectron2.config import get_cfg
from detectron2 import model_zoo

cfg = get_cfg()
#cfg._open_cfg(config_file)
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("layout_train",)
cfg.DATASETS.TEST = ('layout_test',)
cfg.INPUT.MAX_SIZE_TEST = 1247
cfg.INPUT.MAX_SIZE_TRAIN = 1247
cfg.INPUT.MIN_SIZE_TEST = 743
cfg.INPUT.MIN_SIZE_TRAIN = (743,)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 2
#cfg.TEST.EVAL_PERIOD = 500
cfg.EARLY_STOPPING_MONITOR = 'val_loss'
cfg.EARLY_STOPPING_PATIENCE = 10
cfg.SOLVER.BASE_LR = 0.0003  # pick a good LR
cfg.SOLVER.MAX_ITER = 10000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset
cfg.SOLVER.STEPS = []        # do not decay learning rate
cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[1.0,5.0,15.0]]
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)
cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.7
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4   # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.

cfg.OUTPUT_DIR= "maskrcnn_coco_synthetic_output"
#cfg.SOLVER.CHECKPOINT_PERIOD = 2000
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
